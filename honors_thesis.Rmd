---
title: "honors_thesis"
author: "Georges Ip"
date: "3/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Import libraries
library(lattice)
library(MASS)
library(car)
require(stats)
require(stats4)
library(KernSmooth)
library(cluster)
library(leaps)
library(mgcv)
library(rpart)
library(mgcv)
require("datasets")
require(graphics)
library(nlstools)
library(fpp)
library(fpp2)
library(strucchange)
library(Quandl)
library(zoo)
library(PerformanceAnalytics)
library(quantmod)
library(vars)
library(lmtest)
library(tseries)
library(dplyr)
library(lubridate)
library(TTR)
library(repmis)
library(readr)
library(SentimentAnalysis)
library(tm)
library(readxl)
library(tidytext)
library(wordcloud)
library(doParallel)
library(caret)
library(imputeTS)
library(randomForest)
library(gbm)
library(kernlab)
library(e1071)
```

Main stock dataset
```{r}
#using quantmod
sp500 <- new.env()
getSymbols("^GSPC", env = sp500, src = "yahoo", from = as.Date("2009-01-01"), to = as.Date("2019-12-31"))
GSPC <- sp500$GSPC
#Add columns for returns
#frist create a name vector for new returns
names <- c("GSPC.Open.returns", "GSPC.High.returns", "GSPC.Low.returns", "GSPC.Close.returns", "GSPC.Volume.returns", "GSPC.Adjusted.returns")
returns.data<-ROC(GSPC)
names(returns.data) <- names
#Drop volume returns as it is not applicable in this case
returns.data <- subset(returns.data, select = -c(5))
#now combine the two datasets
GSPC<-merge(GSPC,returns.data,join='left') 
#Take first difference
GSPC$diff <- GSPC$GSPC.Adjusted - Lag(GSPC$GSPC.Adjusted)
#Convert difference into output vector
GSPC$direction <- ifelse(GSPC$diff  > 0, 1, 0)
GSPC$direction<-as.factor(GSPC$direction)
#alternative dataset
GSPC2<- GSPC
```

Technical Indicators (1)
```{r}
#implement a simple 10 day moving average
GSPC$SMA10 <- SMA(GSPC$GSPC.Close, n = 10)
#implement a weighted 10 day moving average
GSPC$WMA10 <- WMA(GSPC$GSPC.Adjusted, n = 10, wts = 1:10)
#Implement momentum, the amount the price has changed over the last n periods
momentum = diff(GSPC$GSPC.Adjusted, lag = 4, differences = 1)
GSPC$momentum <- momentum
#Implement stochastic K%, another indicator comparing the closing price of an asset to a range of its prices over a certain period of time n.
#We choose n = 14 for now
GSPC$stochastic.k <- stoch(GSPC[,c("GSPC.High","GSPC.Low", "GSPC.Adjusted")], nFastK = 14, bounded = TRUE)*100
#Implement stochastic D%, which is the 3-day mooving average of K%
GSPC$stochastic.d <- stoch(GSPC[,c("GSPC.High","GSPC.Low", "GSPC.Adjusted")],  nFastD = 3, bounded = TRUE)*100
#Implement RSI, the relative strength index. RSI is a measure of ratio of the recent upward price movements to the absolute price movement
#We will use n=14 for now
GSPC$RSI <- RSI(GSPC$GSPC.Adjusted, n = 14)
#Implement MACD, moving average convergence divergence. This is another oscillator indicator that utilizes exponential moving averages to chart thet trend of price movements 
GSPC$MACD <- MACD(GSPC[,"GSPC.Close"], 12, 26, 9, maType = "EMA")
#Implement Larry Williamâ€™s R%. This is a momentum indicator that tracks overbought and oversold levels. It is quite similar to the stochastic K%.
GSPC$Williams.R <- WPR(GSPC[,c("GSPC.High","GSPC.Low", "GSPC.Close")])
#Implement the A/D (Accumulation/Distribution) Oscillator is an indicator that aims to measure bullish and bearish price pressure. 
GSPC$Williams.AD <- williamsAD(GSPC[,c("GSPC.High","GSPC.Low", "GSPC.Close")])
#Implement the CCI (Commodity Channel Index). The CCI is another momentum indicator that is used to determine when an asset is reaching overbought or oversold conditions. 
GSPC$CCI <- CCI(GSPC[,c("GSPC.High","GSPC.Low", "GSPC.Close")], n = 3, maType = "SMA", c = 0.015)
```

Technical Indicators (2)
```{r}

#Implement OBV, On Balance Volume. OBV is a momentum indicator that tracks changes in volume traded as well as price. 
GSPC2$OBV <- OBV(GSPC2[,"GSPC.Close"], GSPC2[,"GSPC.Volume"])
#Implement a 5 period simple moving average
GSPC2$SMA5 <- SMA(GSPC2$GSPC.Adjusted, n = 5)
#Implement a 6 period bias indicator. This indicatoor derives a directional bias relative tot he 6 period moving average value
SMA6 <- SMA(GSPC2$GSPC.Adjusted, n = 6)
GSPC2$BIAS6 <- (GSPC2$GSPC.Close - SMA6)/SMA6 * 100
#Implement PSY12, which measures the ratio of rising periods over the last 12 periods

GSPC2$PSY12 <- rep(NA,nrow(GSPC))
for(i in 12:nrow(GSPC2)){
GSPC2$PSY12[i] <- sum(GSPC2$direction[(i-11):i]) /12 *100
}
#Create a dummy variable that counts the number of 1's in the last 12 periods
psy.count <- GSPC2$direction
#GSPC$PSY12 <- 
#Implement ASYn, which is the average return over the past n periods
GSPC2$ASY1 <- lag(GSPC2$GSPC.Close.returns, n=1)
GSPC2$ASY2 <- SMA(GSPC2$GSPC.Close.returns, n =2)
GSPC2$ASY3 <- SMA(GSPC2$GSPC.Close.returns, n =3)
GSPC2$ASY4 <- SMA(GSPC2$GSPC.Close.returns, n =4)
GSPC2$ASY5 <- SMA(GSPC2$GSPC.Close.returns, n =5)
```

Fundamental Indicators
```{r}
#Import the China-US exchange rate
USCHEX <- source_data("https://raw.githubusercontent.com/georgesip/financial_engineering/master/DEXCHUS-edited.csv")
USCHEX <- head(USCHEX, -1)
#Combine it with the GSPC dataset
GSPC$USCHEX <- USCHEX$DEXCHUS
#Add DJIA indicator as another fundamental
getSymbols("^DJI", src = "yahoo", from = as.Date("2009-01-01"), to = as.Date("2019-12-31"))
DJIA <- ROC(DJI)
GSPC$DJIA.returns <- DJIA$DJI.Adjusted
```

Sentiment Indicators
```{r}
#Import dataset
news1 <- read_csv("2020-3-6-15-7-23-569723201808043-S&P 500 News - Investing.com-ScrapingData-ScrapeStorm.csv")
news2 <- read_excel("2020-3-7-20-35-11-59646935038951-S&P 500 News - Investing.com-ScrapingData-ScrapeStorm-2.xlsx")
news.1 <- head(news1, -1)
news.full <- rbind(news.1,news2)
```

```{r}
#Clean the dataset
date.fill <- "- Mar 05, 2020"
news.full$date[1:33] <- date.fill
news.full$date <- gsub("- ", "", news.full$date)
news.full$date <- mdy(news.full$date)
drops <- c('Title_link','js-external-link', 'articleDetails')
news.full<-news.full[,!(names(news.full) %in% drops)]
#rearrange chronological order
news.full <- arrange(news.full, date)
#clean the text itself from headlines
news.full$Title <- tolower(news.full$Title)
news.full$Title <- gsub("global markets-", "",news.full$Title)
news.full$Title <- gsub("-", " ",news.full$Title)
news.full$Title <- gsub("[[:punct:]]", " ", news.full$Title)
#clean the text itself from description
news.full$textDiv <- tolower(news.full$textDiv)
news.full$textDiv <- gsub("\\s*\\([^\\)]+\\)","",news.full$textDiv)
news.full$textDiv <- gsub("-", " ",news.full$textDiv)
news.full$textDiv <- gsub("[[:punct:]]", " ", news.full$textDiv)
#crete alternate set wiitout stop words
news.full.alt <- news.full
stopWords <- stopwords("SMART")
news.full.alt$Title<- as.character(news.full.alt$Title) 
'%nin%' <- Negate('%in%')
news.full.alt$Title_clean<-lapply(news.full.alt$Title, function(x) {
  chk <- unlist(strsplit(x," "))
  p <- chk[chk %nin% stopWords]
  paste(p,collapse = " ")
})
news.full.alt$desc_clean<-lapply(news.full.alt$textDiv, function(x) {
  chk <- unlist(strsplit(x," "))
  p <- chk[chk %nin% stopWords]
  paste(p,collapse = " ")
})
#Convert to Corpus
#headlines <- Corpus(VectorSource(news.full$Title))
#descriptions <- Corpus(VectorSource(news.full$textDiv))
```

Summary of data
```{r}
#Wordcloud
wordcloud(news.full$Title,min.freq = 10,colors=brewer.pal(8, "Dark2"),random.color = TRUE,max.words = 500)
Title_clean.df <- as.character(news.full.alt$Title_clean)
wordcloud(Title_clean.df,min.freq = 10,colors=brewer.pal(8, "Dark2"),random.color = TRUE,max.words = 500)
```

Convert to sentiment
```{r}
data(DictionaryLM)
#For headlines
sentiment <- analyzeSentiment(news.full$Title, language = "english", aggregate = news.full$date)
title.sentiment <- sentiment$SentimentLM
news.full <- cbind(news.full, title.sentiment)
#for descriptioin as a sanity check
sentiment2 <- analyzeSentiment(news.full$textDiv, language = "english", aggregate = news.full$date)
desc.sentiment <- sentiment2$SentimentLM
news.full <- cbind(news.full, desc.sentiment)
```

It seems that when we run the sentiment analysis on the description, we obtain greater granular detail. Whereas the short length of the headlines results in multiple zero entries.

Integrate sentiment data with GSPC
```{r}
#First, group by date
news.sentiment.title <- aggregate(news.full["title.sentiment"], by=news.full["date"], sum)
news.sentiment.desc <- aggregate(news.full["desc.sentiment"], by=news.full["date"], sum)
```

```{r}
gspc_sentiment_title <- rep(NA,nrow(GSPC))
gspc_sentiment_desc <- rep(NA,nrow(GSPC))

for(i in 1:nrow(GSPC)){
  if(any(as.Date.character(index(GSPC)[i])==news.sentiment.title$date)){
         gspc_sentiment_title[i] <- news.sentiment.title$title.sentiment[which(as.Date.character(index(GSPC)[i])==news.sentiment.title$date)]
  }
  if(any(as.Date.character(index(GSPC)[i])==news.sentiment.desc$date)){
         gspc_sentiment_desc[i] <- news.sentiment.desc$desc.sentiment[which(as.Date.character(index(GSPC)[i])==news.sentiment.desc$date)]
  }
  
}
GSPC$gspc_sentiment_title <- gspc_sentiment_title
GSPC$gspc_sentiment_desc <- gspc_sentiment_desc
```



We have to offset our predictors as they are ordered time series. Hence, we aim to use predictors calculated at time t, to predict the direction at tme t+1.
```{r}
full.GSPC <- GSPC
#lag predictors by 1 day
for (i in 14:32){
  GSPC[,i] <- lag(GSPC[,i],1)
}
```



We cannot perform the usual resampling methods suhc as LOOCV or k-fold CV as our data is ordered by time. However, the caret library allows for this by using a rolling window estimation method. 
```{r}
#We have to first preprocess the data, as there are huge gaps in the news dataset.
#subset GSPC (missing news before 2014-03-25)
GSPC <- GSPC[1315:2767,]
#convert to dataframe for methods that require it
set.seed(123)
gspc.clean.df <- fortify.zoo(GSPC)
gspc.clean.df$direction <- factor(as.numeric(gspc.clean.df$direction == 2))
rownames(gspc.clean.df) <- gspc.clean.df[,1]
gspc.clean.df <- gspc.clean.df[,-1]
levels(gspc.clean.df$direction) <- c("down", "up")
#Remove all non-relevant variables from GSPC
drops <- c("GSPC.Open","GSPC.High","GSPC.Low","GSPC.Close","GSPC.Volume","GSPC.Adjusted","GSPC.Open.returns","GSPC.High.returns", "GSPC.Low.returns","GSPC.Close.returns", "GSPC.Adjusted.returns", "diff")
GSPC.clean<-gspc.clean.df[ , !(names(gspc.clean.df) %in% drops)]
#Play around with which variables result in greatest accuracy. Start by copying Kara's paper
drops2 <- c("gspc_sentiment_title")
GSPC.clean<-gspc.clean.df[ , !(names(gspc.clean.df) %in% drops2)]
#Assess the number of NA's
colSums(is.na(GSPC.clean))
#There are still some NA's in USCHEX, we can do some imputation
GSPC.clean <- na.locf(GSPC.clean)
#prepare parallel computing method
registerDoParallel(cores=6)
#Split into training and test set
GSPC.end <- floor(0.8*nrow(GSPC.clean)) #select the first 80% of the data
GSPC.train <- GSPC.clean[1:GSPC.end,] #assign the first 80% of the data to the train set
GSPC.test <- GSPC.clean[(GSPC.end+1):nrow(gspc.clean.df),]
```


Prepare for caret library
```{r}
set.seed(1)
myTimeControl <- trainControl(method = "timeslice",
                              initialWindow = 32,
                              horizon = 4,
                              fixedWindow = FALSE,
                              classProbs = TRUE, 
                              summaryFunction = twoClassSummary,
                              allowParallel = TRUE)
```


LASSO/RIDGE
```{r}
#optimise using AIC and BIC

lasso.grid <- expand.grid(lambda=seq(0,1,by=0.1),alpha=1)
lasso.fit <- train(direction ~ .,
                    data = GSPC.train,
                    method = "glmnet",
                    family = "binomial",
                    preProcess = c("range"),
                    trControl = myTimeControl,
                    tuneGrid = lasso.grid)
lasso.pred <- predict(lasso.fit, GSPC.test)
confusionMatrix(lasso.pred, GSPC.test$direction)
mean(lasso.pred == GSPC.test$direction)                    
```


Random Forest
```{r}
#a quick way to find a good value for mtry
#bestMtry <- tuneRF(GSPC.train[,2:ncol(GSPC.train)], GSPC.train$direction, stepFactor = 1.5, improve = 1e-5, ntree = 1000)
#Using caret to grid search
rf.control <-trainControl(method = "timeslice",
                              initialWindow = 32,
                              horizon = 12,
                              fixedWindow = FALSE,
                              number = 10,
                              classProbs = TRUE, 
                              allowParallel = TRUE)
mtry <- sqrt(ncol(GSPC.train))
rfgrid <- expand.grid(.mtry=mtry)
rf.fit <- train(direction ~ .,
                    data = GSPC.train,
                    method = "rf",
                    metric = 'Accuracy',
                    preProcess = c("range"),
                    trControl = rf.control,
                    tuneGrid = rfgrid)
print(rf.fit)
rf.test <- predict(rf.fit, newdata = GSPC.test)
confusionMatrix(rf.test, GSPC.test$direction)
#Alternatively
tune_grid <- expand.grid(
  mtry = c(
    ncol(GSPC.train)-1, # p
    ncol((GSPC.train)-1)/ 3, # p / 3
    ceiling(sqrt(ncol(GSPC.train))) # square root of p
  )
)
rf.control2 <-trainControl(method = "timeslice",
                              initialWindow = 32,
                              horizon = 12,
                              fixedWindow = FALSE,
                              number = 10,
                              search = "random",
                              classProbs = TRUE, 
                              allowParallel = TRUE,
                              repeats = 3)
rf.fit2 <- train(direction ~ .,
                    data = GSPC.train,
                    method = "rf",
                    metric = 'Accuracy',
                    preProcess = c("range"),
                    trControl = rf.control2,
                    tuneGrid = tune_grid)
print(rf.fit2)
rf.test2 <- predict(rf.fit2, newdata = GSPC.test)
confusionMatrix(rf.test2, GSPC.test$direction)
```


SVM
```{r}
#First we find the optimal tuning parameter
trainX <- GSPC.train[,2:29]
svm.tune <- train(direction~.,
                  data = GSPC.train,
                  method = "svmRadial",   # Radial kernel
                  tuneLength = 9,					# 9 values of the cost function
                  preProc = c("range"),  # Center and scale data
                  metric="ROC",
                  trControl=myTimeControl)
svm.tune
#we narrow the grid search
svm.grid <- expand.grid(sigma = c(.040, .036, 0.030), C = c(0.25, 0.2, 0.35))
svm.tune2 <- train(x=trainX,
                  y= GSPC.train$direction,
                  method = "svmRadial",   # Radial kernel
                  tuneGrid = svm.grid,					# 9 values of the cost function
                  preProc = c("range"),  # Center and scale data
                  metric="ROC",
                  trControl=myTimeControl)
svm.test <- predict(svm.tune, newdata = GSPC.test)
confusionMatrix(svm.test, GSPC.test$direction)
```




